# AskMe Voice Assistant Configuration

# Model configurations
models:
  # Large Language Model settings
  llm:
    name: "mistral-7b-instruct"  # Base model name
    path: "./models/mistral-7b-instruct-gguf"  # Local model path
    model_file: "askme-q4_0.gguf"  # Specific model file
    context_length: 4096  # Maximum context window
    temperature: 0.7  # Response creativity (0.0-1.0)
    top_p: 0.9  # Nucleus sampling parameter
    top_k: 40  # Top-k sampling parameter
    max_tokens: 512  # Maximum response length
    device: "auto"  # Device selection: auto, cpu, cuda, mps
    gpu_layers: 35  # Number of layers to offload to GPU
    threads: -1  # CPU threads (-1 for auto)
    batch_size: 512  # Batch size for processing
    
  # Automatic Speech Recognition settings
  asr:
    model: "base"  # Whisper model size: tiny, base, small, medium, large
    language: "en"  # Primary language (null for auto-detection)
    task: "transcribe"  # transcribe or translate
    device: "auto"  # cpu or cuda
    compute_type: "float16"  # float16, float32, int8
    beam_size: 5  # Beam search width
    best_of: 5  # Number of candidates
    patience: 1.0  # Beam search patience
    length_penalty: 1.0  # Length penalty for beam search
    repetition_penalty: 1.0  # Repetition penalty
    no_speech_threshold: 0.6  # Silence detection threshold
    logprob_threshold: -1.0  # Log probability threshold
    compression_ratio_threshold: 2.4  # Compression ratio threshold
    condition_on_previous_text: true  # Use previous text as context
    
  # Text-to-Speech settings
  tts:
    model: "tts_models/en/ljspeech/tacotron2-DDC"  # Default TTS model
    vocoder: "vocoder_models/en/ljspeech/hifigan_v2"  # Vocoder model
    speaker: null  # Default speaker (null for single-speaker models)
    language: "en"  # Language for multilingual models
    emotion: "neutral"  # Emotion for emotional TTS models
    speed: 1.0  # Speech speed multiplier
    device: "auto"  # cpu or cuda

# Audio settings
audio:
  sample_rate: 16000  # Audio sample rate (Hz)
  chunk_size: 1024  # Audio chunk size for streaming
  channels: 1  # Number of audio channels (1=mono, 2=stereo)
  format: "float32"  # Audio format: int16, int32, float32
  
  # Voice Activity Detection
  vad:
    enabled: true  # Enable voice activity detection
    aggressiveness: 3  # VAD aggressiveness (0-3)
    frame_duration_ms: 30  # Frame duration in milliseconds
    
  # Input settings
  input:
    device_index: null  # Audio input device index (null for default)
    energy_threshold: 300  # Energy threshold for speech detection
    dynamic_energy_threshold: true  # Automatically adjust energy threshold
    pause_threshold: 0.8  # Seconds of silence to consider end of phrase
    phrase_timeout: 5.0  # Seconds to timeout listening
    
  # Output settings
  output:
    device_index: null  # Audio output device index (null for default)
    volume: 0.8  # Output volume (0.0-1.0)

# User Interface settings
ui:
  host: "localhost"  # Web interface host
  port: 8080  # Web interface port
  theme: "dark"  # UI theme: light, dark, auto
  title: "AskMe Voice Assistant"  # Application title
  
  # Features
  features:
    voice_input: true  # Enable voice input interface
    text_input: true  # Enable text input interface
    conversation_history: true  # Show conversation history
    model_selection: true  # Allow model switching
    settings_panel: true  # Show settings panel
    
  # Security
  security:
    cors_origins: ["http://localhost:3000", "http://127.0.0.1:3000"]  # CORS origins
    api_key: null  # API key for authentication (null to disable)

# Privacy and data settings
privacy:
  store_conversations: false  # Store conversation history locally
  conversation_retention_days: 7  # Days to keep conversations (if storing)
  anonymize_logs: true  # Remove personal info from logs
  telemetry_enabled: false  # Disable telemetry collection
  
# Logging configuration
logging:
  level: "INFO"  # Log level: DEBUG, INFO, WARNING, ERROR
  file: "logs/askme.log"  # Log file path
  max_file_size: "10MB"  # Maximum log file size
  backup_count: 5  # Number of backup log files
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
# Performance settings
performance:
  max_concurrent_requests: 5  # Maximum concurrent requests
  request_timeout: 30  # Request timeout in seconds
  memory_optimization: true  # Enable memory optimizations
  cpu_optimization: true  # Enable CPU optimizations
  
  # Caching
  cache:
    enabled: true  # Enable response caching
    size: 100  # Cache size (number of entries)
    ttl: 3600  # Cache TTL in seconds

# Development settings
development:
  debug: false  # Enable debug mode
  hot_reload: false  # Enable hot reload for development
  profiling: false  # Enable performance profiling
  mock_audio: false  # Use mock audio input for testing

# Advanced settings
advanced:
  # Custom prompts and behaviors
  system_prompt: |
    You are AskMe, a helpful offline voice assistant that prioritizes user privacy.
    You operate entirely locally without internet connectivity.
    Be conversational, helpful, and concise in your responses.
    When you cannot perform internet-dependent tasks, suggest offline alternatives.
  
  # Response filtering
  response_filters:
    max_length: 1000  # Maximum response length in characters
    filter_profanity: true  # Filter inappropriate content
    filter_personal_info: true  # Filter potential personal information
    
  # Plugin system
  plugins:
    enabled: []  # List of enabled plugins
    disabled: []  # List of disabled plugins
    search_paths: ["./plugins"]  # Plugin search paths

# Model-specific optimizations
optimizations:
  # Quantization settings
  quantization:
    enabled: true  # Enable model quantization
    method: "q4_0"  # Quantization method
    
  # Memory management
  memory:
    low_memory_mode: false  # Enable low memory mode
    swap_to_cpu: true  # Swap unused model parts to CPU
    garbage_collection: true  # Enable aggressive garbage collection
    
  # Threading
  threading:
    max_workers: 4  # Maximum thread pool workers
    enable_parallel_processing: true  # Enable parallel processing where possible
